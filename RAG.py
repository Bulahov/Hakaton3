#!/usr/bin/env python
# coding: utf-8

# In[1]:


# from sentence_transformers import CrossEncoder
# import torch

# # –£–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ –≤–∞—à–µ–π –ª–æ–∫–∞–ª—å–Ω–æ–π –ø–∞–ø–∫–µ —Å —Ä–µ—Ä–∞–Ω–∫–µ—Ä–æ–º
# RERANKER_PATH = r"bge_model" 
# # –ò–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏, –µ—Å–ª–∏ –∫–∞—á–∞–µ—Ç–µ –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞
# # RERANKER_PATH = "BAAI/bge-reranker-v2-m3"

# device = 'cuda' if torch.cuda.is_available() else 'cpu'
# print(f"–ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å –∏–∑: {RERANKER_PATH}")

# model = CrossEncoder(RERANKER_PATH, device=device)

# # –î–≤–∞ –ø—Ä–∏–º–µ—Ä–∞: –æ–¥–∏–Ω —è–≤–Ω–æ –ø–æ–¥—Ö–æ–¥–∏—Ç, –≤—Ç–æ—Ä–æ–π - –Ω–µ—Ç
# query = "–ö–∞–∫ —Å–æ–∑–¥–∞—Ç—å –≤–∏—Ä—Ç—É–∞–ª—å–Ω—É—é –º–∞—à–∏–Ω—É?"
# good_doc = "–î–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π –º–∞—à–∏–Ω—ã –ø–µ—Ä–µ–π–¥–∏—Ç–µ –≤ —Ä–∞–∑–¥–µ–ª Compute Cloud –∏ –Ω–∞–∂–º–∏—Ç–µ –°–æ–∑–¥–∞—Ç—å."
# bad_doc = "–†–µ—Ü–µ–ø—Ç –±–æ—Ä—â–∞: –≤–æ–∑—å–º–∏—Ç–µ —Å–≤–µ–∫–ª—É, –∫–∞–ø—É—Å—Ç—É –∏ –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å."

# scores = model.predict([
#     [query, good_doc],
#     [query, bad_doc]
# ])

# print(f"\nScore —Ö–æ—Ä–æ—à–∏–π –¥–æ–∫—É–º–µ–Ω—Ç: {scores[0]:.4f}")
# print(f"Score –ø–ª–æ—Ö–æ–π –¥–æ–∫—É–º–µ–Ω—Ç: {scores[1]:.4f}")

# if scores[0] > scores[1] and scores[0] > 0:
#     print("\n‚úÖ –í–°–Å –†–ê–ë–û–¢–ê–ï–¢! –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –º–æ–∂–Ω–æ –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å.")
# else:
#     print("\n‚ùå –ú–û–î–ï–õ–¨ –°–õ–û–ú–ê–ù–ê. –û–Ω–∞ –Ω–µ —Ä–∞–∑–ª–∏—á–∞–µ—Ç —Ç–µ–∫—Å—Ç—ã. –ù—É–∂–Ω–æ –ø–µ—Ä–µ–∫–∞—á–∞—Ç—å.")


# In[2]:


import os
import re
import json
import logging
import numpy as np
from typing import List, Dict
import chromadb
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer, CrossEncoder
from rank_bm25 import BM25Okapi
from openai import OpenAI


# –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É logs
os.makedirs("logs", exist_ok=True)

# –°–æ–∑–¥–∞—ë–º –ª–æ–≥–≥–µ—Ä
bot_logger = logging.getLogger("rag_log")
bot_logger.setLevel(logging.INFO)

# –°–æ–∑–¥–∞—ë–º —Ñ–∞–π–ª–æ–≤—ã–π —Ö–µ–Ω–¥–ª–µ—Ä
file_handler = logging.FileHandler("logs/rag.log", encoding="utf-8")
file_handler.setLevel(logging.INFO)

# –§–æ—Ä–º–∞—Ç –ª–æ–≥–æ–≤
formatter = logging.Formatter(
    "%(asctime)s ‚Äî %(levelname)s ‚Äî %(name)s ‚Äî %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)
file_handler.setFormatter(formatter)

# –ü—Ä–∏–≤—è–∑—ã–≤–∞–µ–º —Ö–µ–Ω–¥–ª–µ—Ä
bot_logger.addHandler(file_handler)


# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
BASE_DIR = "NeuroEmotions_hackathon3_cloud_ru_data2"
VECTOR_DB_PATH = os.path.join(BASE_DIR, "vector_db", "chroma_db_e5_correct")
# –§–∞–π–ª —Å –ø–æ–ª–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏ –Ω—É–∂–µ–Ω –¥–ª—è BM25
RAG_JSON_PATH = os.path.join(BASE_DIR,"rag_ready", "NeuroEmotions_all_chunks_for_rag_1.6.json")
FULL_TUTORIALS_PATH = os.path.join(BASE_DIR, "json", "NeuroEmotions_all_tutorials_1.6.json")
# –ö–ª—é—á–∏ –∏ –º–æ–¥–µ–ª–∏
load_dotenv()
# CLOUD_RU_API_KEY = os.getenv("CLOUD_RU_API_KEY")
CLOUD_RU_API_KEY = ""
CLOUD_RU_BASE_URL = "https://foundation-models.api.cloud.ru/v1"
CLOUD_RU_MODEL = "ai-sage/GigaChat3-10B-A1.8B"
#–ï–º–±–µ–¥–¥–µ—Ä –∏ —Ä–µ—Ä–∞–Ω–∫–µ—Ä
EMBEDDING_MODEL = "multilingual-e5-small"
#RERANKER_MODEL = "bge_model"
#RERANKER_MODEL = "reranker_model_mini"
RERANKER_MODEL = "reranker_model_tiny"

#–ö–ª–∞—Å—Å RAG–∞
class AdvancedRAG:
    def __init__(self):
        print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Advanced RAG System...")

        # 1. –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ API LLM
        self.llm_client = OpenAI(api_key=CLOUD_RU_API_KEY, base_url=CLOUD_RU_BASE_URL)

        # 2. –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ (ChromaDB + E5)
        print("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ ChromaDB –∏ E5...")
        self.chroma_client = chromadb.PersistentClient(path=VECTOR_DB_PATH)
        self.collection = self.chroma_client.get_collection("neuroemotions_e5_correct")
        self.embedder = SentenceTransformer(EMBEDDING_MODEL)

        # 3. –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (BM25)
        print("üìñ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è BM25 (—ç—Ç–æ –∑–∞–π–º–µ—Ç –ø–∞—Ä—É —Å–µ–∫—É–Ω–¥)...")
        self.documents_cache = [] # –•—Ä–∞–Ω–∏–º —Ç—É—Ç —Ç–µ–∫—Å—Ç—ã –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        self._init_bm25()

        # 4. Reranker
        print("‚öñÔ∏è –ó–∞–≥—Ä—É–∑–∫–∞ Reranker (Cross-Encoder)...")
        self.reranker = CrossEncoder(RERANKER_MODEL)

        # 5. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞ –∫–æ–¥–æ–≤ (Code Registry)
        print("üß© –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞ –∫–æ–¥–æ–≤...")
        self._init_code_registry()

        print("‚úÖ –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")

    def _init_bm25(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∏ —Å—Ç—Ä–æ–∏–º –∏–Ω–¥–µ–∫—Å BM25"""
        with open(RAG_JSON_PATH, 'r', encoding='utf-8') as f:
            data = json.load(f)

        self.documents_cache = data # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø–∞–º—è—Ç–∏, —á—Ç–æ–±—ã –¥–æ—Å—Ç–∞–≤–∞—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ

        # —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è: —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ –ø—Ä–æ–±–µ–ª–∞–º
        tokenized_corpus = [doc['text'].lower().split(" ") for doc in data]
        self.bm25 = BM25Okapi(tokenized_corpus)
    def _init_code_registry(self):
        """
        –°–æ–∑–¥–∞–µ–º –±—ã—Å—Ç—Ä—ã–π —Å–ª–æ–≤–∞—Ä—å: {URL -> [–°–ø–∏—Å–æ–∫ –±–ª–æ–∫–æ–≤ –∫–æ–¥–∞]}
        –ß—Ç–æ–±—ã –±—ã—Å—Ç—Ä–æ –º–µ–Ω—è—Ç—å [[CODE_BLOCK_N]] –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–π –∫–æ–¥.
        """
        self.code_registry = {}
        try:
            with open(FULL_TUTORIALS_PATH, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # data['tutorials'] —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π
                tutorials = data.get('tutorials', [])

            for tut in tutorials:
                url = tut['metadata']['url']
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–æ–∫ –±–ª–æ–∫–æ–≤ –∫–æ–¥–∞ –¥–ª—è —ç—Ç–æ–≥–æ URL
                self.code_registry[url] = tut.get('code_blocks', [])

            print(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∫–æ–¥–æ–≤ –¥–ª—è {len(self.code_registry)} —Å—Ç–∞—Ç–µ–π.")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞ –∫–æ–¥–æ–≤: {e}")
            print("   –§—É–Ω–∫—Ü–∏—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∫–æ–¥–∞ —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–µ –±—É–¥–µ—Ç.")

    def _restore_code_blocks(self, text: str, url: str) -> str:
        """
        –ò—â–µ—Ç [[CODE_BLOCK_N]] –∏ –∑–∞–º–µ–Ω—è–µ—Ç –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–π –∫–æ–¥.
        """
        if "[[CODE_BLOCK_" not in text:
            return text

        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –∫–æ–¥–æ–≤ –¥–ª—è —ç—Ç–æ–π —Å—Ç–∞—Ç—å–∏
        code_blocks = self.code_registry.get(url, [])
        if not code_blocks:
            return text

        # –§—É–Ω–∫—Ü–∏—è –∑–∞–º–µ–Ω—ã –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–Ω–æ–≥–æ –≤—ã—Ä–∞–∂–µ–Ω–∏—è
        def replace_match(match):
            try:
                index = int(match.group(1)) # –ü–æ–ª—É—á–∞–µ–º —á–∏—Å–ª–æ N –∏–∑ CODE_BLOCK_N
                if 0 <= index < len(code_blocks):
                    block = code_blocks[index]
                    code_content = block['text']
                    lang = block.get('language', '')
                    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∫—Ä–∞—Å–∏–≤–æ –¥–ª—è LLM
                    return f"\n```{lang}\n{code_content}\n```\n"
                return match.group(0) # –ï—Å–ª–∏ –∏–Ω–¥–µ–∫—Å–∞ –Ω–µ—Ç, –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
            except:
                return match.group(0)

        # –ó–∞–º–µ–Ω—è–µ–º –≤—Å–µ –≤—Ö–æ–∂–¥–µ–Ω–∏—è [[CODE_BLOCK_(\d+)]]
        restored_text = re.sub(r'\[\[CODE_BLOCK_(\d+)\]\]', replace_match, text)
        return restored_text

    def hybrid_search(self, query: str, top_k_vector=10, top_k_keyword=10) -> List[Dict]:
        """
        –®–∞–≥ 1: –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ (Vector + Keyword)
        """
        # --- –ê. –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ ---
        query_vec = self.embedder.encode([f"query: {query}"]).tolist()
        vec_res = self.collection.query(query_embeddings=query_vec, n_results=top_k_vector)

        vector_candidates = []
        if vec_res['documents']:
            for i in range(len(vec_res['documents'][0])):
                vector_candidates.append({
                    'text': vec_res['documents'][0][i],
                    'metadata': vec_res['metadatas'][0][i],
                    'source': 'vector'
                })

        # --- –ë. –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (BM25) ---
        tokenized_query = query.lower().split(" ")
        # BM25 –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—Å—Ç—ã, –Ω–∞–º –Ω—É–∂–Ω–æ –Ω–∞–π—Ç–∏ –∏—Ö –∏–Ω–¥–µ–∫—Å—ã –∏–ª–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        # rank_bm25 –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω–¥–µ–∫—Å—ã –Ω–∞–ø—Ä—è–º—É—é, –ø–æ—ç—Ç–æ–º—É:
        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø N –ª—É—á—à–∏—Ö –∏–Ω–¥–µ–∫—Å–æ–≤
        scores = self.bm25.get_scores(tokenized_query)
        top_n_indexes = np.argsort(scores)[::-1][:top_k_keyword]

        keyword_candidates = []
        for idx in top_n_indexes:
            doc = self.documents_cache[idx]
            keyword_candidates.append({
                'text': doc['text'],
                'metadata': doc['metadata'],
                'source': 'bm25'
            })

        # --- –í. –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è ---
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—Å—Ç –∫–∞–∫ –∫–ª—é—á —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
        unique_docs = {}

        for doc in vector_candidates + keyword_candidates:
            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏ (—Ö—ç—à –æ—Ç —Ç–µ–∫—Å—Ç–∞)
            doc_hash = hash(doc['text'])
            if doc_hash not in unique_docs:
                unique_docs[doc_hash] = doc

        print(f"   üîç Hybrid Search: –ù–∞–π–¥–µ–Ω–æ {len(unique_docs)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ (Vec={len(vector_candidates)}, BM25={len(keyword_candidates)})")
        return list(unique_docs.values())

    def rerank(self, query: str, candidates: List[Dict], top_k=5) -> List[Dict]:
        """
        –®–∞–≥ 2: –ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        """
        if not candidates:
            return []

        # –ì–æ—Ç–æ–≤–∏–º –ø–∞—Ä—ã [Query, Document Text]
        pairs = [[query, doc['text']] for doc in candidates]

        # –ü–æ–ª—É—á–∞–µ–º –æ—Ü–µ–Ω–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        scores = self.reranker.predict(pairs)

        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ü–µ–Ω–∫–∏ –∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
        for i, doc in enumerate(candidates):
            doc['score'] = scores[i]

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é –æ—Ü–µ–Ω–∫–∏
        sorted_candidates = sorted(candidates, key=lambda x: x['score'], reverse=True)

        # –ë–µ—Ä–µ–º —Ç–æ–ø-K
        final_results = sorted_candidates[:top_k]

        print(f"   ‚öñÔ∏è Reranker: –í—ã–±—Ä–∞–Ω–æ —Ç–æ–ø-{top_k} –ª—É—á—à–∏—Ö. –õ—É—á—à–∏–π score: {final_results[0]['score']:.4f}")
        return final_results

    def generate_with_check(self, query: str, context: List[Dict]) -> str:
        """
        –®–∞–≥ 3 –∏ 4: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å —Å–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫–æ–π
        """
        # –°–±–æ—Ä–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        context_str = ""
        for i, item in enumerate(context, 1):
            meta = item['metadata']
            raw_text = item['text']
            url = meta.get('source_url', '')

            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–π –∫–æ–¥ –≤–º–µ—Å—Ç–æ [[CODE_BLOCK]]
            clean_text = self._restore_code_blocks(raw_text, url)

            # –§–æ—Ä–º–∏—Ä—É–µ–º –∞—Ç—Ä–∏–±—É—Ç—ã –¥–ª—è —Ç–µ–≥–∞ <doc>
            attrs = [f'id="{i}"', f'source="{meta.get("source_title", "Unknown")}"']

            if 'category' in meta:
                attrs.append(f'category="{meta["category"]}"')

            # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–≥
            attr_str = " ".join(attrs)
            context_str += f'<{attr_str}>\n{clean_text}\n</doc>\n'

        # --- –ì–ï–ù–ï–†–ê–¶–ò–Ø –û–¢–í–ï–¢–ê ---
        system_prompt = """
–¢—ã ‚Äî AI-—Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä –ø–æ –ø–ª–∞—Ç—Ñ–æ—Ä–º–µ Cloud.ru Evolution.
–¢–≤–æ—è —Ü–µ–ª—å ‚Äî –ø–æ–º–æ–≥–∞—Ç—å —Å—Ç—É–¥–µ–Ω—Ç–∞–º –æ—Å–≤–∞–∏–≤–∞—Ç—å –æ–±–ª–∞—á–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –æ–±—ä—è—Å–Ω—è—Ç—å —Ç–µ—Ä–º–∏–Ω—ã –∏ –¥–∞–≤–∞—Ç—å –ø—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞.

–ü–†–ê–í–ò–õ–ê –û–¢–í–ï–¢–ê:
1. –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –ö–æ–Ω—Ç–µ–∫—Å—Ç. –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π —Ñ—É–Ω–∫—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ Cloud.ru.
2. –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –µ—Å—Ç—å –ø—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞, –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –≤–∫–ª—é—á–∏ –∏—Ö –≤ –æ—Ç–≤–µ—Ç.
3. –û—Ç–≤–µ—á–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–π Markdown (–∂–∏—Ä–Ω—ã–π —à—Ä–∏—Ñ—Ç, —Å–ø–∏—Å–∫–∏, –±–ª–æ–∫–∏ –∫–æ–¥–∞).
4. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, —Ç–∞–∫ –∏ —Å–∫–∞–∂–∏: "–í –º–æ–∏—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö –Ω–µ—Ç –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å".
5. –¢–æ–Ω: –î—Ä—É–∂–µ–ª—é–±–Ω—ã–π, –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π, –ø–µ–¥–∞–≥–æ–≥–∏—á–µ—Å–∫–∏–π.
6. –ü–µ—Ä–µ–¥ —Ç–µ–º –∫–∞–∫ –¥–∞—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –∏–ª–∏ –∫–æ–¥, –≤—ã–ø–∏—à–∏ –≤ –Ω–∞—á–∞–ª–µ —Ç–æ—á–Ω—É—é —Ü–∏—Ç–∞—Ç—É –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–π –æ—Å–Ω–æ–≤–∞–Ω —Ç–≤–æ–π –æ—Ç–≤–µ—Ç. –ù–∞—á–∏–Ω–∞–π –æ—Ç–≤–µ—Ç —Å–æ —Å–ª–æ–≤: '–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: ...
"""
        user_prompt = f"–í–æ–ø—Ä–æ—Å: {query}\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context_str}"

        print("   ü§ñ –ì–µ–Ω–µ—Ä–∏—Ä—É—é —á–µ—Ä–Ω–æ–≤–∏–∫ –æ—Ç–≤–µ—Ç–∞...")
        draft_response = self._call_llm(system_prompt, user_prompt)

        # --- –°–ê–ú–û–ü–†–û–í–ï–†–ö–ê (SELF-CORRECTION) ---
        print("   üïµÔ∏è –°–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ç–≤–µ—Ç–∞...")
        verify_prompt = f"""
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –æ—Ç–≤–µ—Ç –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π.
–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context_str}

–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {query}
–ß–µ—Ä–Ω–æ–≤–∏–∫ –æ—Ç–≤–µ—Ç–∞: {draft_response}

–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è:
1. –ü—Ä–æ–≤–µ—Ä—å, –≤—Å–µ –ª–∏ —Ñ–∞–∫—Ç—ã –≤ "–ß–µ—Ä–Ω–æ–≤–∏–∫–µ –æ—Ç–≤–µ—Ç–∞" –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç—Å—è "–ö–æ–Ω—Ç–µ–∫—Å—Ç–æ–º".
2. –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–æ–π –ù–ï–¢ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤—ã–¥—É–º–∞–Ω–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã, –ø–∞—Ä–∞–º–µ—Ç—Ä—ã) ‚Äî –ò–°–ü–†–ê–í–¨ –µ–≥–æ.
3. –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç –≤–µ—Ä–Ω—ã–π –∏ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ ‚Äî –≤–µ—Ä–Ω–∏ –µ–≥–æ –ë–ï–ó –∏–∑–º–µ–Ω–µ–Ω–∏–π.
4. –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞.
"""
        final_response = self._call_llm("–¢—ã ‚Äî —Å—Ç—Ä–æ–≥–∏–π —Ä–µ–¥–∞–∫—Ç–æ—Ä —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏.", verify_prompt)

        return final_response

    def _call_llm(self, sys_prompt, user_prompt):
        try:
            response = self.llm_client.chat.completions.create(
                model=CLOUD_RU_MODEL,
                messages=[
                    {"role": "system", "content": sys_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.1, # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
                max_tokens=5000
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"–û—à–∏–±–∫–∞ LLM: {e}"

    def ask(self, query: str):
        print(f"\n–í–æ–ø—Ä–æ—Å: {query}")
        bot_logger.info(f"–í–æ–ø—Ä–æ—Å: {query}")

        # 1. –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ (Recall)
        # –ë–µ—Ä–µ–º –ø–æ 15 –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –æ—Ç –∫–∞–∂–¥–æ–≥–æ –º–µ—Ç–æ–¥–∞, —á—Ç–æ–±—ã –±—ã–ª–æ –∏–∑ —á–µ–≥–æ –≤—ã–±–∏—Ä–∞—Ç—å
        candidates = self.hybrid_search(query, top_k_vector=15, top_k_keyword=15)
        bot_logger.info(f"–û—Ç–æ–±—Ä–∞–Ω—ã –∫–∞–Ω–¥–∏–¥–∞—Ç—ã")

        if not candidates:
            return "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."

        # 2. Reranking
        # –û—Å—Ç–∞–≤–ª—è–µ–º 5 —Å–∞–º—ã—Ö –ª—É—á—à–∏—Ö –∫—É—Å–∫–æ–≤
        best_docs = self.rerank(query, candidates, top_k=5)
        bot_logger.info(f"–û—Ç–æ–±—Ä–∞–Ω—ã –ª—É—á—à–∏–µ –∫—É—Å–∫–∏")

        # –î–ª—è –æ—Ç–ª–∞–¥–∫–∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º, —á—Ç–æ –≤—ã–±—Ä–∞–ª–∏
        print(f"   üèÜ –¢–æ–ø –¥–æ–∫—É–º–µ–Ω—Ç: {best_docs[0]['metadata'].get('source_title')} (Score: {best_docs[0]['score']:.2f})")

        # 3. –°–±–æ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        # –ú—ã —Å–æ–±–∏—Ä–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –∏–∑ —Ç–æ–ø-5 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        sources = []
        seen_urls = set()
        for doc in best_docs:
            title = doc['metadata'].get('source_title', '–î–æ–∫—É–º–µ–Ω—Ç')
            url = doc['metadata'].get('source_url', '#')
            category = doc['metadata'].get('category', '')

            if url not in seen_urls and url != '#':
                sources.append({
                    "title": title,
                    "url": url,
                    "category": category
                })
                seen_urls.add(url)
        bot_logger.info(f"–ü—Ä–æ–∏–∑–≤–µ–¥–µ–Ω —Å–±–æ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")

        # 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è + –ü—Ä–æ–≤–µ—Ä–∫–∞
        answer = self.generate_with_check(query, best_docs)
        bot_logger.info(f"–ü—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è + –ø—Ä–æ–≤–µ—Ä–∫–∞")

        return answer, sources

        
    def generate_questions(self, question: str, n: int = 3) -> List[str]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç n –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è —Å–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞.

        Args:
            question (str): –ò—Å—Ö–æ–¥–Ω—ã–π –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
            n (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 3).

        Returns:
            List[str]: –°–ø–∏—Å–æ–∫ –∏–∑ n –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è —Å–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫–∏.
        """
        print(f"\n‚ùì –ì–µ–Ω–µ—Ä–∞—Ü–∏—è {n} –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è —Å–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ —Ç–µ–º–µ: {question}")

        # 1. –ü–æ–ª—É—á–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ ask)
        candidates = self.hybrid_search(question, top_k_vector=10, top_k_keyword=10)
        if not candidates:
            return [f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã –ø–æ —Ç–µ–º–µ: {question}"]

        best_docs = self.rerank(question, candidates, top_k=3)

        # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç (–±–µ–∑ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∫–æ–¥–∞, –µ—Å–ª–∏ –Ω–µ –Ω—É–∂–Ω–æ ‚Äî –Ω–æ –ª—É—á—à–µ —Å –Ω–∏–º)
        context_str = ""
        for i, item in enumerate(best_docs, 1):
            meta = item['metadata']
            raw_text = item['text']
            url = meta.get('source_url', '')
            clean_text = self._restore_code_blocks(raw_text, url)
            context_str += f"<doc id=\"{i}\" source=\"{meta.get('source_title', 'Unknown')}\">\n{clean_text}\n</doc>\n"

        # 2. –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤
        system_prompt = (
            "–¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å –ø–æ –æ–±–ª–∞—á–Ω—ã–º —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è–º Cloud.ru. "
            "–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è —Å–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–Ω–∞–Ω–∏–π —Å—Ç—É–¥–µ–Ω—Ç–∞. "
            "–í–æ–ø—Ä–æ—Å—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —á—ë—Ç–∫–∏–º–∏, –æ—Ö–≤–∞—Ç—ã–≤–∞—Ç—å –∫–ª—é—á–µ–≤—ã–µ –ø–æ–Ω—è—Ç–∏—è –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –ø–æ–º–æ–≥–∞—Ç—å –∑–∞–∫—Ä–µ–ø–∏—Ç—å –º–∞—Ç–µ—Ä–∏–∞–ª. "
            "–§–æ—Ä–º—É–ª–∏—Ä—É–π –≤–æ–ø—Ä–æ—Å—ã –≤ —Å—Ç–∏–ª–µ —ç–∫–∑–∞–º–µ–Ω–∞—Ü–∏–æ–Ω–Ω—ã—Ö –∏–ª–∏ —É—á–µ–±–Ω—ã—Ö –∑–∞–¥–∞–Ω–∏–π."
        )

        user_prompt = (
            f"–ò—Å—Ö–æ–¥–Ω—ã–π –≤–æ–ø—Ä–æ—Å —Å—Ç—É–¥–µ–Ω—Ç–∞: \"{question}\"\n\n"
            f"–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context_str}\n\n"
            f"–ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–π —Ä–æ–≤–Ω–æ {n} –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è —Å–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫–∏. "
            f"–ö–∞–∂–¥—ã–π –≤–æ–ø—Ä–æ—Å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏ –∏ –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å ¬´- ¬ª (–º–∏–Ω—É—Å –∏ –ø—Ä–æ–±–µ–ª). "
            f"–ù–µ –¥–æ–±–∞–≤–ª—è–π –ø–æ—è—Å–Ω–µ–Ω–∏–π, —Ç–æ–ª—å–∫–æ —Å–ø–∏—Å–æ–∫ –≤–æ–ø—Ä–æ—Å–æ–≤."
        )

        # 3. –í—ã–∑—ã–≤–∞–µ–º LLM
        try:
            response = self.llm_client.chat.completions.create(
                model=CLOUD_RU_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.5,  # –Ω–µ–º–Ω–æ–≥–æ –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
                max_tokens=500
            )
            raw_output = response.choices[0].message.content.strip()
        except Exception as e:
            return [f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤: {e}"]

        # 4. –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç: –æ–∂–∏–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤ —Ñ–æ—Ä–º–∞—Ç–µ "- –í–æ–ø—Ä–æ—Å 1\n- –í–æ–ø—Ä–æ—Å 2..."
        questions = []
        for line in raw_output.split("\n"):
            if line.strip().startswith("- "):
                q = line.strip()[2:].strip()
                if q:
                    questions.append(q)
            # –¢–∞–∫–∂–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –Ω–∞ —Å–ª—É—á–∞–π –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –æ—Ç —Ñ–æ—Ä–º–∞—Ç–∞
            elif line.strip() and line.strip()[0].isdigit() and ". " in line:
                q = line.split(". ", 1)[1].strip()
                if q:
                    questions.append(q)

        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å ‚Äî –≤–µ—Ä–Ω—ë–º –∫–∞–∫ –µ—Å—Ç—å, —Ä–∞–∑–±–∏–≤ –ø–æ —Å—Ç—Ä–æ–∫–∞–º
        if not questions:
            questions = [q.strip() for q in raw_output.split("\n") if q.strip()]

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ n –≤–æ–ø—Ä–æ—Å–æ–≤
        return questions[:n]


    def recommend_materials(self, topic: str, n: int = 3) -> List[Dict[str, str]]:
        """
        –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç n —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —É—á–µ–±–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –ø–æ –∑–∞–¥–∞–Ω–Ω–æ–π —Ç–µ–º–µ.

        Args:
            topic (str): –¢–µ–º–∞, –ø–æ –∫–æ—Ç–æ—Ä–æ–π –Ω—É–∂–Ω—ã –º–∞—Ç–µ—Ä–∏–∞–ª—ã.
            n (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 3).

        Returns:
            List[Dict[str, str]]: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∫–ª—é—á–∞–º–∏ 'title', 'url', 'category'.
        """
        print(f"\nüìö –ü–æ–∏—Å–∫ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –ø–æ —Ç–µ–º–µ: {topic}")

        # 1. –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        candidates = self.hybrid_search(topic, top_k_vector=15, top_k_keyword=15)
        if not candidates:
            return []

        # 2. –ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        best_docs = self.rerank(topic, candidates, top_k=n)

        # 3. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ ask)
        recommended = []
        seen_urls = set()

        for doc in best_docs:
            meta = doc['metadata']
            title = meta.get('source_title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')
            url = meta.get('source_url', '')
            category = meta.get('category', '–û–±—â–µ–µ')

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏ –ø–æ URL
            if url in seen_urls or not url or url == '#':
                continue

            recommended.append({
                'title': title,
                'url': url,
                'category': category
            })
            seen_urls.add(url)

            if len(recommended) >= n:
                break

        # –ï—Å–ª–∏ –Ω–µ –Ω–∞–±—Ä–∞–ª–æ—Å—å n —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö ‚Äî –¥–æ–ø–æ–ª–Ω–∏–º –∏–∑ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –±–µ–∑ rerank (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
        if len(recommended) < n:
            for doc in candidates:
                if len(recommended) >= n:
                    break
                meta = doc['metadata']
                url = meta.get('source_url', '')
                if url in seen_urls or not url or url == '#':
                    continue
                recommended.append({
                    'title': meta.get('source_title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'),
                    'url': url,
                    'category': meta.get('category', '–û–±—â–µ–µ')
                })
                seen_urls.add(url)

        return recommended[:n]


# --- –ó–ê–ü–£–°–ö ---
if __name__ == "__main__":
    if "–í–ê–®_–ö–õ–Æ–ß" in CLOUD_RU_API_KEY:
        print("‚ö†Ô∏è –û–®–ò–ë–ö–ê: –í—Å—Ç–∞–≤—å—Ç–µ API KEY!")
    else:
        bot = AdvancedRAG()

        # –°–ª–æ–∂–Ω—ã–π –≤–æ–ø—Ä–æ—Å, –≥–¥–µ –Ω—É–∂–µ–Ω –∏ –∫–æ–¥, –∏ —Ç–µ—Ä–º–∏–Ω—ã
        q = "–ö–∞–∫ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –º–æ–¥–µ–ª–∏ –Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö?"

        ans = bot.ask(q)
        print("\n" + "="*50)
        print("üéì –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–í–ï–¢:")
        print(ans['answer'])
        print("\nüìö –ò–°–¢–û–ß–ù–ò–ö–ò:")
        for src in ans['sources']:
            print(f"üîó {src['title']} ({src['category']})")
            print(f"   {src['url']}")
        print("="*50)

        questions = bot.generate_questions("–ö–∞–∫ —Å–æ–∑–¥–∞—Ç—å –≤–∏—Ä—Ç—É–∞–ª—å–Ω—É—é –º–∞—à–∏–Ω—É –≤ Cloud.ru?", n=5)
        for q in questions:
            print(f"- {q}")

        materials = bot.recommend_materials("–†–∞–±–æ—Ç–∞ —Å –æ–±—ä–µ–∫—Ç–Ω—ã–º —Ö—Ä–∞–Ω–∏–ª–∏—â–µ–º –≤ Cloud.ru", n=5)
        for m in materials:
            print(f"- {m['title']} ({m['category']})\n  {m['url']}\n")

